{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ee\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "import geemap\n",
    "\n",
    "from geeml.utils import eeprint\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/geethen/fire/Burn_Area_Mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "service_account = 'github-action@ee-geethensingh.iam.gserviceaccount.com'\n",
    "credentials = ee.ServiceAccountCredentials(service_account, r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\secret.json\")\n",
    "ee.Initialize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.data_extraction import extractDataset\n",
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = geemap.Map()\n",
    "fc = ee.FeatureCollection(\"projects/ee-geethensingh/assets/postdoc/proj_fired_south_africa_to2021182_events\")\n",
    "Map.addLayer(ee.Feature(fc.first()), {'color': 'red'}, \"Fire Test\")\n",
    "Map.centerObject(fc.first(), 12)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter fire events to period and area of interest\n",
    "fireEvents = fc.map(lambda ft: ft.set('system:time_start', ee.Date(ft.get('ig_date')))\n",
    "                    .set('system:time_end', ee.Date(ft.get('last_date')))).filterDate('2016-01-01', '2017-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Sentinel-2 features for South Africa - error with or before getstats function. returns None values\n",
    "# extractDataset('Sentinel-2', 'South Africa', ee.Date('2021-01-01'), ee.Date('2021-01-02'), fireEvents, 5, 'extract_s2_2021010120220101.csv')\n",
    "# Get Landsat-8 features for South Africa (24 hrs to extract data for 1 year. 1 day = 9 Landsat images = 4 mins)\n",
    "extractDataset('LANDSAT_8', 'South Africa', ee.Date('2016-01-01'), ee.Date('2017-01-01'), fireEvents, 16, 'extract_l8_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/geethen/fire/Burn_Area_Mapping\")\n",
    "from src.components.data_ingestion import dataIngestion\n",
    "from src.components.data_transformation import dataTransformation\n",
    "from src.components.model_trainer import modelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = dataIngestion()\n",
    "# train_path, calibration_path, test_path = obj.initiate_data_ingestion()\n",
    "from pathlib import Path\n",
    "import os\n",
    "train_path = os.path.join(Path.cwd().parent,'components/artifacts',\"train.csv\")\n",
    "test_path = os.path.join(Path.cwd().parent,'components/artifacts',\"test.csv\")\n",
    "calibration_path = os.path.join(Path.cwd().parent,'components/artifacts',\"calibration.csv\")\n",
    "\n",
    "data_transformation = dataTransformation()\n",
    "train_arr, cal_arr, test_arr = data_transformation.initiate_data_transformation(train_path, calibration_path, test_path)\n",
    "\n",
    "model_trainer = modelTrainer()\n",
    "model_trainer.initiate_model_trainer(train_arr, cal_arr, test_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_object\n",
    "import pandas as pd\n",
    "\n",
    "from mapie.classification import MapieClassifier\n",
    "\n",
    "# model = load_object(r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping\\src\\components\\artifacts\\model.pkl\")\n",
    "df = pd.read_csv(r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping\\src\\components\\artifacts\\train.csv\")\n",
    "\n",
    "\n",
    "X_train = df.drop(['scenes', 'label'], axis=1)\n",
    "y_train = df[['label']]\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state= 42, verbose= False).fit(X_train, y_train)\n",
    "print(model.predict(dfPredict))\n",
    "\n",
    "mapie_score = MapieClassifier(estimator = model, cv=\"prefit\", method=\"lac\", random_state= 42)\n",
    "mapie_score.fit(X_train, y_train)\n",
    "mapie_score.predict(dfPredict, alpha =0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import Inference\n",
    "\n",
    "inference_pipeline = Inference()\n",
    "sceneList = inference_pipeline.initiate_inference_pipeline('LANDSAT_8', 'South Africa')\n",
    "sceneList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Delete logs older than March 1, 2024\n",
    "delete_old_logs(r'C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping', datetime.datetime(2024, 3, 11))\n",
    "# Or, delete logs older than one month (default behavior)\n",
    "delete_old_logs(r'C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ee\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "import geemap\n",
    "\n",
    "from geeml.utils import eeprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"C:/Users/coach/myfiles/postdoc/Fire/code/Burn_Area_Mapping\")\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from stage_2.inference import segment\n",
    "\n",
    "# perform inference\n",
    "segment().main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supportedSensors = {'Sentinel-1': 'COPERNICUS/S1_GRD',\n",
    "                    'Sentinel-2': \"COPERNICUS/S2_SR_HARMONIZED\",\n",
    "                    'LANDSAT_8': \"LANDSAT/LC08/C02/T1_L2\",\n",
    "                    'LANDSAT_9': \"LANDSAT/LC09/C02/T1_L2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_object\n",
    "import pandas as pd\n",
    "\n",
    "downloadList = pd.read_pickle(r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping\\src\\components\\artifacts\\downloadList.pkl\")\n",
    "downloadList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(sensor: str, pre: bool, event: ee.Feature, dateBuffer: int)->ee.Image:\n",
    "    \"\"\"\n",
    "    Get the temporally closest pre-fire and post-fire optical or radar images after preprocessing both\n",
    "    the optical images.\n",
    "\n",
    "    Args:\n",
    "        event (ee.Feature): A fire event delineating the boundary of burnt area\n",
    "        dateBuffer (int): The temporal window to locate a relevant image. In units of weeks.\n",
    "\n",
    "    Returns:\n",
    "        A pre-fire and post-fire optical or radar image\n",
    "    \"\"\"\n",
    "    # Filter images that intersect the geometry using the optimised filterBounds. Thereafter, use the\n",
    "    # ee.filter.contains that is not as optimised as filterBounds\n",
    "    images = supportedSensors.get(sensor).filterBounds(event.geometry())\n",
    "    eventDate = ee.Date(event.get('Ig_Date'))\n",
    "    if pre:\n",
    "        startDate = eventDate.advance(dateBuffer*-1, 'week')\n",
    "        endDate = eventDate\n",
    "        outImage = cloudMask1(images.filterDate(startDate, endDate).sort('system:time_start', False)).reduce(ee.Reducer.firstNonNull()).regexpRename('.{6}$','')\n",
    "        # .filter(ee.Filter.contains('.geo', event.geometry())).sort('system:time_start', False).first()\n",
    "\n",
    "    else:\n",
    "        # postfire\n",
    "        startDate = eventDate\n",
    "        endDate = eventDate.advance(dateBuffer, 'week')\n",
    "        outImage = cloudMask2(images.filterDate(startDate, endDate))#.reduce(ee.Reducer.firstNonNull()).regexpRename('.{6}$','')#.qualityMosaic('nbr')\n",
    "        # .filter(ee.Filter.contains('.geo', event.geometry())).sort('system:time_start', True).first()\n",
    "    \n",
    "    return outImage\n",
    "\n",
    "    # Given a fire event get pre and post fire image from multi-sensors\n",
    "def getOpticalRadarPairs(sensor:str, dnbr:bool, event: ee.Feature, dateBuffer: int = 4)-> ee.Image:\n",
    "    \"\"\"\n",
    "    Get optical-radar pairs for a given fire event. First image after fire event and image before fire event\n",
    "\n",
    "    Args:\n",
    "        sensor(str): The name of the sensor. One of ['Sentinel-1', 'Sentinel-2', 'Landsat_8', 'Landsat_9']\n",
    "        event(ee.Feature): A fire event\n",
    "        dateBuffer(int): The temporal-search window. \n",
    "    \n",
    "    Returns:\n",
    "        An image with optical-radar pairs\n",
    "\n",
    "    \"\"\"\n",
    "    # Post-fire image\n",
    "    if sensor != 'any':\n",
    "        postOpticalImage = getImage(sensor, False, event, dateBuffer= dateBuffer)\n",
    "    else:\n",
    "        s2post = getImage('Sentinel-2', False, event, dateBuffer= dateBuffer)\n",
    "        l8post = getImage('LANDSAT_8', False, event, dateBuffer= dateBuffer)\n",
    "        l9post = getImage('LANDSAT_9', False, event, dateBuffer= dateBuffer)\n",
    "        # get the earliest optical image (between L8, L9 and S2) that was captured after the fire event\n",
    "        postOpticalImage = ee.ImageCollection.fromImages([s2post, l8post, l9post]).sort('system:time_start', True).first()\n",
    "    # pre-process post-fire image\n",
    "    # postOpticalImage = ee.Image(preprocessOptical(postOpticalImage, sensor).copyProperties(postOpticalImage))\n",
    "\n",
    "    # if sensor == 'Sentinel-1':\n",
    "    #     # RADAR\n",
    "    #     s1post = getImage('Sentinel-1', False, event, dateBuffer= dateBuffer)\n",
    "    #     # preprocess pre-fire radar image\n",
    "    #     parameters.update({'STOP_DATE': s1post.date().advance(1, 'day'), 'START_DATE': s1post.date(), 'ROI': event.geometry()})\n",
    "    #     s1post = wp.s1_preproc(parameters).select('V.')\n",
    "    \n",
    "    # pre-fire\n",
    "    if sensor != 'any':\n",
    "        # If only one sensor is to be used\n",
    "        preOpticalImage = getImage(sensor, True, event, dateBuffer= dateBuffer)\n",
    "    else:\n",
    "        # If any optical sensor is to be used\n",
    "        s2pre = getImage('Sentinel-2', True, event, dateBuffer= dateBuffer)\n",
    "        l8pre = getImage('LANDSAT_8', True, event, dateBuffer= dateBuffer)\n",
    "        l9pre = getImage('LANDSAT_9', True, event, dateBuffer= dateBuffer)\n",
    "        # get the earliest optical image (between L8, L9 and S2) that was captured after the fire event\n",
    "        preOpticalImage = ee.ImageCollection.fromImages([s2pre, l8pre, l9pre]).sort('system:time_start', True).first()\n",
    "    \n",
    "    # elif multisensor:\n",
    "    #     # if multisensor data is to be used, add radar to optical\n",
    "    #     # RADAR\n",
    "    #     s1pre = getImage('Sentinel-1', True, event, dateBuffer= dateBuffer)\n",
    "    #     # preprocess pre-fire radar image\n",
    "    #     parameters.update({'STOP_DATE': s1pre.date().advance(1, 'day'), 'START_DATE': s1pre.date(), 'ROI': event.geometry()})\n",
    "    #     s1pre = wp.s1_preproc(parameters).select('V.')\n",
    "\n",
    "    # pre-process pre-fire image\n",
    "    # preOpticalImage = ee.Image(preprocessOptical(preOpticalImage, sensor).copyProperties(preOpticalImage))\n",
    "\n",
    "    if dnbr:\n",
    "        postOpticalImage = qualityMosaicPercentile(postOpticalImage.map(lambda img: img.addBands(\n",
    "            preOpticalImage.normalizedDifference(['B8A', 'B12']).subtract(img.normalizedDifference(['B8A', 'B12'])).rename('dnbr'))), 'dnbr', 95) \n",
    "    \n",
    "    return postOpticalImage\n",
    "    # return preOpticalImage.regexpRename('$', '_pre'), s1pre, postOpticalImage.regexpRename('$', '_post'), s1post\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fires = ee.FeatureCollection(\"projects/ee-geethensingh/assets/postdoc/All_fires_23_24_gw\").filter(ee.Filter.gte('YEAR', 2024))\\\n",
    ".map(lambda ft: ft.set('area', ft.area())).sort('area', False)\n",
    "eeprint(fires.first())\n",
    "img = ee.ImageCollection(supportedSensors.get('Sentinel-2')).filterBounds(fires.first().geometry()).filterDate('2024-01-22', '2024-02-28').sort('CLOUD_COVERAGE_ASSESSMENT', True).first()\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = geemap.Map()\n",
    "Map.addLayer(img, vis_params = {'bands': ['B12', 'B8', 'B4'], 'gamma': 1.0, 'min': 1221.8729292334365, 'max': 2294.690588294787})\n",
    "Map.addLayer(fires.first().geometry())\n",
    "Map.centerObject(img, zoom=10)\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which scenes need to be downloaded\n",
    "from src.utils import load_object\n",
    "import ee\n",
    "from geeml.utils import eeprint\n",
    "from geedim.download import BaseImage\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "# ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# supportedSensors = {'Sentinel-1': ee.ImageCollection('COPERNICUS/S1_GRD'),\n",
    "#                     'Sentinel-2': ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\"),\n",
    "#                     'LANDSAT_8': ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\"),\n",
    "#                     'LANDSAT_9': ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\")}\n",
    "\n",
    "supportedSensors = {'Sentinel-1': 'COPERNICUS/S1_GRD',\n",
    "                    'Sentinel-2': \"COPERNICUS/S2_SR_HARMONIZED\",\n",
    "                    'LANDSAT_8': \"LANDSAT/LC08/C02/T1_L2\",\n",
    "                    'LANDSAT_9': \"LANDSAT/LC09/C02/T1_L2\"}\n",
    "\n",
    "def preprocessOptical(image, sensor):\n",
    "    # Step 1) select bands\n",
    "    bandSelect = {'Sentinel-2': 'B.*',\n",
    "                    'LANDSAT_8': 'SR_B.',\n",
    "                    'LANDSAT_9': 'SR_B.'}\n",
    "    # Step 2) radiometric correction\n",
    "    radiometricMultiplySelect = {'Sentinel-2': 0.0001,\n",
    "                    'LANDSAT_8': 0.0000275,\n",
    "                    'LANDSAT_9': 0.0000275}\n",
    "    radiometricAddSelect = {'Sentinel-2': 0,\n",
    "                    'LANDSAT_8': -0.2,\n",
    "                    'LANDSAT_9': -0.2}\n",
    "    processed = image.select(bandSelect.get(sensor)).multiply(radiometricMultiplySelect.get(sensor)).add(radiometricAddSelect.get(sensor))\n",
    "\n",
    "    # Step 3) Cloud mask\n",
    "\n",
    "    # Step 4) Compute NBR\n",
    "    if sensor == 'Sentinel-2':\n",
    "        wIndices = processed.addBands([processed.normalizedDifference(['B8A', 'B12']).rename('nbr'), processed.normalizedDifference(['B8', 'B4']).rename('ndvi')])\n",
    "    if sensor == 'LANDSAT_8' or sensor == 'LANDSAT_9':\n",
    "        wIndices = processed.addBands([processed.normalizedDifference(['B5', 'B7']).rename('nbr'), processed.normalizedDifference(['B5', 'B4']).rename('ndvi')])\n",
    "    return wIndices\n",
    "\n",
    "def qualityMosaicPercentile(collection, QAband, percentile):\n",
    "    # Compute percentile image\n",
    "    percentileImage = collection.select(QAband).reduce(ee.Reducer.percentile([percentile]))\n",
    "    #  Compute distance of every pixel from the computed percentile in that location\n",
    "    withDist = ee.ImageCollection(collection.map(lambda image: image.addBands([\n",
    "        image.select(QAband).subtract(percentileImage).abs().multiply(-1).rename('quality')])))\n",
    "    return withDist.qualityMosaic('quality')\n",
    "\n",
    "def prepareInferenceImage(imageID: str, sensor:str)-> ee.Image:\n",
    "    \"\"\"Given an Image ID that is predicted (by the scene classification model) to contain a burn area,\n",
    "    download the required input data for the semantic segmentation model\n",
    "    \n",
    "    Args:\n",
    "        imageID (str): earth engine image ID\n",
    "        sensor (str): An optical sensor. Either 'Sentinel-2', 'LANDSAT_8' or 'LANDSAT_9'.\n",
    "        \n",
    "    Returns:\n",
    "        ee.Image with 16 bands for the before and after image\n",
    "        \n",
    "        \"\"\"\n",
    "    eventImage = ee.Image.load(f'{supportedSensors.get(sensor)}/{imageID}')\n",
    "    # radiometric calibration, compute NBR\n",
    "    postImg = preprocessOptical(eventImage, sensor)# mask clouds\n",
    "    eventDate = eventImage.date()\n",
    "    startDate = eventDate.advance(15*-1, 'week')\n",
    "    endDate = eventDate\n",
    "    # Filter to relevant pre-fire images\n",
    "    images = ee.ImageCollection(supportedSensors.get(sensor)).filterBounds(postImg.geometry()).filterDate(startDate, endDate)\n",
    "    # filter images, mask clouds, compute nbr and ndvi, compute compsoite based on 95th percentile NDVI. subtract from postNBR.\n",
    "    if sensor == 'LANDSAT_8' or sensor == 'LANDSAT_9':\n",
    "        preImg = ee.Image(qualityMosaicPercentile(images.map(lambda img: preprocessOptical(\n",
    "            img, sensor)),'ndvi', 95))\n",
    "    else:\n",
    "        preImg = ee.Image(qualityMosaicPercentile(ee.ImageCollection(images.map(lambda img: preprocessOptical(\n",
    "            img, sensor))),'ndvi', 95))\n",
    "\n",
    "    dnbr = preImg.select('nbr').subtract(postImg.select('nbr')).rename('dnbr')\n",
    "    bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8A', 'B11', 'B12', 'nbr', 'dnbr']\n",
    "    return postImg.addBands(dnbr).select(bands)\n",
    "\n",
    "# downloadList = load_object(r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping\\src\\components\\artifacts\\downloadList.pkl\")\n",
    "# downloadList\n",
    "# load image\n",
    "# for img in downloadList[20:21]:\n",
    "#     print(img[2:])\n",
    "#     # download scenes\n",
    "#     eeImg = prepareInferenceImage(img[2:])\n",
    "#     filename = f'C:/Users/coach/myfiles/postdoc/Fire/code/Burn_Area_Mapping/src/components/artifacts/segScenes/{img[2:]}.tif'\n",
    "#     BaseImage(eeImg).download(filename, crs='EPSG:4326', region= eeImg.geometry(), scale=30, overwrite=True, num_threads=20, dtype= 'float64')\n",
    "#     #load model\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     model = smp.Unet(\n",
    "#         encoder_name=\"resnet34\",        \n",
    "#         encoder_weights= None,     \n",
    "#         in_channels=16,                  \n",
    "#         classes=2,  \n",
    "#     ).to(device)\n",
    "#     # r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping\\src\\components\\artifacts\\segModel_22042024.pth\"\n",
    "#     checkpoint = torch.load(r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\models\\UNet_05072024.pth\")\n",
    "#     model.load_state_dict(checkpoint)\n",
    "    \n",
    "    # run inference\n",
    "    # 5min13s to download and 1min to run inference\n",
    "    # upload to gee imagecollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image\n",
    "imgID = '20240219T082009_20240219T084413_T34HCJ'\n",
    "eeImg = prepareInferenceImage(imgID, 'Sentinel-2')\n",
    "filename = f'C:/Users/coach/myfiles/postdoc/Fire/code/Burn_Area_Mapping/src/components/artifacts/segScenes/{imgID}.tif'\n",
    "BaseImage(eeImg).download(filename, crs='EPSG:4326', region= eeImg.geometry(), scale=30, overwrite=True, num_threads=20, dtype= 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Check if CUDA devices are available\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# Check if CUDA drivers are installed\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Check if CUDA is enabled\n",
    "print(torch.backends.cudnn.enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchseg\n",
    "import torch\n",
    "# load model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = torchseg.Unet(\n",
    "    encoder_name=\"convnextv2_tiny\",        \n",
    "    encoder_weights= None,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=14,\n",
    "    classes=2,\n",
    "    encoder_depth= 4,\n",
    "    decoder_channels=(256, 128, 64, 32),\n",
    "    head_upsampling=2                      \n",
    ")\n",
    "    # r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping\\src\\components\\artifacts\\segModel_22042024.pth\"\n",
    "checkpoint = torch.load(r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\models\\FTUnet_imgnet_convnextT_29092024.pth\")\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "Map = geemap.Map()\n",
    "img = \"2_LC08_173078_20240305\"\n",
    "eeimg = ee.Image.load(f'LANDSAT/LC08/C02/T1_TOA/{img[2:]}').select([\"B7\",\"B5\",\"B4\"])\n",
    "Map.addLayer(eeimg, {'bands': ['B7', 'B5', 'B4'], 'min':0, 'max':0.7})\n",
    "Map.centerObject(eeimg.geometry(), 9)\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('c:\\\\Users\\\\coach\\\\myfiles\\\\postdoc\\\\Fire\\\\code\\\\Burn_Area_Mapping\\\\src')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils import load_object\n",
    "# from utils import MyNormalize\n",
    "from torchgeo.transforms import indices, AugmentationSequential\n",
    "\n",
    "# mean, std = load_object(r\"components\\artifacts\\norm_vals.pkl\")\n",
    "# normalize = MyNormalize(mean=mean, stdev=std)\n",
    "\n",
    "# Create transforms\n",
    "data_transform = AugmentationSequential(\n",
    "    indices.AppendNDBI(index_swir=8, index_nir=9),\n",
    "    indices.AppendNDWI(index_green=1, index_nir=6),\n",
    "    indices.AppendNDVI(index_nir=6, index_red=2),\n",
    "    data_keys = ['image']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import rasterio as rio\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import os\n",
    "import ee\n",
    "import threading\n",
    "\n",
    "def inference(\n",
    "    infile: str,\n",
    "    imgTransforms: Callable[[dict], dict],\n",
    "    model: torch.nn.Module,\n",
    "    outfile: str,\n",
    "    patchSize: int,\n",
    "    overlap: int = 16,\n",
    "    num_workers: int = 4,\n",
    "    device: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run inference using model on infile block-by-block and write to a new file (outfile).\n",
    "    If the infile image width/height is not exactly divisible by 32, padding\n",
    "    is added for inference and removed prior to saving the outfile.\n",
    "    \n",
    "    Args:\n",
    "        infile (str): Path to input image/covariates.\n",
    "        imgTransforms (Callable): Function to transform input images.\n",
    "        model (torch.nn.Module): Loaded trained model/checkpoint.\n",
    "        outfile (str): Path to save the predicted image.\n",
    "        patchSize (int): Must be a multiple of 32. Size independent of model input size.\n",
    "        overlap (int): Number of overlapping pixels between patches.\n",
    "        num_workers (int): Number of workers to parallelize across.\n",
    "        device (str, optional): Device to run the model on.\n",
    "        \n",
    "    Returns:\n",
    "        None: A TIFF file is saved to the outfile destination.\n",
    "\n",
    "    # Example usage:\n",
    "        # infile = 'path/to/input.tif'\n",
    "        # imgTransforms = some_torchgeo_transforms_function\n",
    "        # model = some_loaded_pytorch_model\n",
    "        # outfile = 'path/to/output.tif'\n",
    "        # inference(infile, imgTransforms, model, outfile, patchSize=256, overlap=16, num_workers=4, device='cuda')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the input file using rasterio\n",
    "    with rio.open(infile) as src:\n",
    "        # Set up logging\n",
    "        logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Create a destination dataset based on source parameters\n",
    "        profile = src.profile\n",
    "        profile.update(blockxsize=patchSize, blockysize=patchSize, tiled=True, count=1)\n",
    "        \n",
    "        # Open the output file with the updated profile\n",
    "        with rio.open(Path(outfile), \"w\", **profile) as dst:\n",
    "            # Get all windows (patches) in the destination dataset\n",
    "            windows = [window for ij, window in dst.block_windows()]\n",
    "            \n",
    "            # Create locks for reading and writing to ensure thread safety\n",
    "            read_lock = threading.Lock()\n",
    "            write_lock = threading.Lock()\n",
    "            \n",
    "            def process(window: Window) -> None:\n",
    "                \"\"\"\n",
    "                Process a single window (patch) by reading it, transforming it, running the model on it,\n",
    "                and writing the result to the output file.\n",
    "                \"\"\"\n",
    "                # Acquire the read lock to safely read from the input file\n",
    "                with read_lock:\n",
    "                    # edge case- border patches cannot have overlap on atleast one edge\n",
    "                    # so if left and top start is negative, take 0\n",
    "                    # col_off = max(window.col_off - overlap, 0)\n",
    "                    # row_off = max(window.row_off - overlap, 0)\n",
    "\n",
    "                    col_off = window.col_off - overlap\n",
    "                    row_off = window.row_off - overlap\n",
    "                    \n",
    "                    # If top or left point is 0, width is going to include patchsize + overlap on one side.\n",
    "                    # If patch is on right or bottom edge, there may be insufficient pixels for overlap and a patch.\n",
    "                    # In this case, the width will be less than patchsize + overlap\n",
    "                    # Case only relevant for small images< patchSize (i.e., col_off, row_off ==0 and min is not src width/height).\n",
    "                    # Adjust width and height based on overlap and boundaries\n",
    "                    # if col_off == 0:\n",
    "                    #     width = min(patchSize + overlap, src.width - col_off)\n",
    "                    # else:\n",
    "                    #     width = min(patchSize + overlap * 2, src.width - col_off)\n",
    "                    # if row_off == 0:\n",
    "                    #     height = min(patchSize + overlap, src.height - row_off)\n",
    "                    # else:\n",
    "                    #     height = min(patchSize + overlap * 2, src.height - row_off)\n",
    "                    width = patchSize + overlap * 2\n",
    "                    height = patchSize + overlap * 2\n",
    "                    # If patch starts in the middle of the image, it will have overlaps on both sides (overlap*2).\n",
    "                    # Except for right and bottom edge.\n",
    "                    \n",
    "                    # Create a window with overlap\n",
    "                    overlap_window = Window(\n",
    "                        col_off=col_off,\n",
    "                        row_off=row_off,\n",
    "                        width=width,\n",
    "                        height=height\n",
    "                    )\n",
    "                    \n",
    "                    # Read the data from the input file within the overlap window\n",
    "                    src_array = src.read(boundless=True, window=overlap_window, fill_value = 0.0)\n",
    "                    src_array = torch.from_numpy(src_array)\n",
    "\n",
    "                    # Add padding if the patch is on the boundary of the large image\n",
    "                    # pad_left = overlap if col_off == 0 else 0\n",
    "                    # pad_top = overlap if row_off == 0 else 0\n",
    "\n",
    "                    # right and bottom padding is required when width and height are less than patchSize + overlap*2\n",
    "                    # The amount of padding required is equal to patchSize + overlap*2 - width and height\n",
    "                    # pad_right = patchSize+overlap*2 - width if width < patchSize + overlap * 2 else 0\n",
    "                    # pad_bottom = patchSize+overlap*2 - height if height < patchSize + overlap * 2 else 0\n",
    "                    # transform = transforms.Pad((pad_left, pad_top, pad_right, pad_bottom))\n",
    "                    # src_array = transform(src_array)\n",
    "                    \n",
    "                    # Apply the image transformations\n",
    "                    image = imgTransforms({\"image\": src_array})['image']#.squeeze()\n",
    "\n",
    "                    def d4_transformations(image):\n",
    "                        # Generate the eight \\(\\text{D}_4\\) transformations\n",
    "                        transformations = [\n",
    "                            image,                                # Original\n",
    "                            F.rotate(image, 90),                  # Rotate 90 degrees\n",
    "                            F.rotate(image, 180),                 # Rotate 180 degrees\n",
    "                            F.rotate(image, 270),                 # Rotate 270 degrees\n",
    "                            F.hflip(image),                       # Horizontal flip\n",
    "                            F.vflip(image),                       # Vertical flip\n",
    "                            F.vflip(F.rotate(image, 90)),         # Vertical flip + Rotate 90 degrees\n",
    "                            F.hflip(F.rotate(image, 90))          # Horizontal flip + Rotate 90 degrees\n",
    "                        ]\n",
    "                        return torch.stack(transformations)\n",
    "                    \n",
    "                    def inverse_d4_transformations(outputs):\n",
    "                        # Define the inverse transformations for D4\n",
    "                        inverses = [\n",
    "                            lambda x: x,                          # Original\n",
    "                            lambda x: F.rotate(x, -90),           # Rotate -90 degrees\n",
    "                            lambda x: F.rotate(x, -180),          # Rotate -180 degrees\n",
    "                            lambda x: F.rotate(x, -270),          # Rotate -270 degrees\n",
    "                            lambda x: F.hflip(x),                 # Horizontal flip\n",
    "                            lambda x: F.vflip(x),                 # Vertical flip\n",
    "                            lambda x: F.rotate(F.vflip(x), -90),  # Rotate -90 degrees after vertical flip\n",
    "                            lambda x: F.rotate(F.hflip(x), -90)   # Rotate -90 degrees after horizontal flip\n",
    "                        ]\n",
    "                        # Apply each inverse transformation to the corresponding output\n",
    "                        return torch.stack([inverses[i](output) for i, output in enumerate(outputs)])\n",
    "\n",
    "\n",
    "                    # Apply D4 transformations to create a batch\n",
    "                    # d4_batch = d4_transformations(image)\n",
    "\n",
    "                    # Move batch to the device\n",
    "                    # d4_batch = d4_batch.to(device, dtype=torch.float)\n",
    "                    image = image.to(device, dtype=torch.float)\n",
    "\n",
    "                    # Set the model to evaluation mode\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                            # Run the model on the padded image\n",
    "                            output = model(torch.nan_to_num(image, nan=0.0, neginf=0.0, posinf=0.0))\n",
    "\n",
    "                            # Apply the inverse transformations\n",
    "                            # outputs = inverse_d4_transformations(output)\n",
    "                            \n",
    "                            # Take the mean of the batch along the batch dimension\n",
    "                            # output = outputs.mean(dim=0)\n",
    "\n",
    "                    # Remove overlap and/or padding from the output\n",
    "                    # ws = max(window.col_off - overlap, 0)\n",
    "                    # rs = max(window.row_off - overlap, 0)\n",
    "                    result = output[:, 1, overlap:patchSize + overlap, overlap:patchSize + overlap].squeeze().detach().cpu()\n",
    "                    # overlap:patchSize + overlap, overlap:\n",
    "                # Acquire the write lock to safely write to the output file\n",
    "                with write_lock:\n",
    "                    dst.write(result.numpy(), 1, window=window)\n",
    "            \n",
    "            # Use a ThreadPoolExecutor to process the windows in parallel\n",
    "            with tqdm(total=len(windows), desc=os.path.basename(outfile)) as pbar:\n",
    "                with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                    futures = {executor.submit(process, window): window for window in windows}\n",
    "                    \n",
    "                    try:\n",
    "                        for future in as_completed(futures):\n",
    "                            future.result()  # Wait for the future to complete\n",
    "                            pbar.update(1)  # Update the progress bar\n",
    "                    except Exception as ex:\n",
    "                        logger.error('Error during inference: %s', ex)\n",
    "                        executor.shutdown(wait=False, cancel_futures=True)\n",
    "                        raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import rasterio as rio\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import os\n",
    "import ee\n",
    "import threading\n",
    "\n",
    "def inference(\n",
    "    infile: str,\n",
    "    imgTransforms: Callable[[dict], dict],\n",
    "    model: torch.nn.Module,\n",
    "    outfile: str,\n",
    "    patchSize: int = 256,\n",
    "    overlap: int = 128,\n",
    "    num_workers: int = 1,\n",
    "    device: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run inference using model on infile block-by-block and write to a new file (outfile).\n",
    "    If the infile image width/height is not exactly divisible by 32, padding\n",
    "    is added for inference and removed prior to saving the outfile.\n",
    "    \n",
    "    Args:\n",
    "        infile (str): Path to input image/covariates.\n",
    "        imgTransforms (Callable): Function to transform input images.\n",
    "        model (torch.nn.Module): Loaded trained model/checkpoint.\n",
    "        outfile (str): Path to save the predicted image.\n",
    "        patchSize (int): Must be a multiple of 32. Size independent of model input size.\n",
    "        overlap (int): Number of overlapping pixels between patches.\n",
    "        num_workers (int): Number of workers to parallelize across.\n",
    "        device (str, optional): Device to run the model on.\n",
    "        \n",
    "    Returns:\n",
    "        None: A TIFF file is saved to the outfile destination.\n",
    "\n",
    "    # Example usage:\n",
    "        # infile = 'path/to/input.tif'\n",
    "        # imgTransforms = some_torchgeo_transforms_function\n",
    "        # model = some_loaded_pytorch_model\n",
    "        # outfile = 'path/to/output.tif'\n",
    "        # inference(infile, imgTransforms, model, outfile, patchSize=256, overlap=16, num_workers=4, device='cuda')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the input file using rasterio\n",
    "    with rio.open(infile) as src:\n",
    "        # Set up logging\n",
    "        logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Create a destination dataset based on source parameters\n",
    "        profile = src.profile\n",
    "        profile.update(blockxsize=patchSize, blockysize=patchSize, tiled=True, count=1, compress = 'lzw')\n",
    "        \n",
    "        # Open the output file with the updated profile\n",
    "        with rio.open(Path(outfile), \"w\", **profile) as dst:\n",
    "            # Get all windows (patches) in the destination dataset\n",
    "            windows = [window for ij, window in dst.block_windows()]\n",
    "            \n",
    "            # Create locks for reading and writing to ensure thread safety\n",
    "            read_lock = threading.Lock()\n",
    "            write_lock = threading.Lock()\n",
    "            \n",
    "            def process(window: Window) -> None:\n",
    "                \"\"\"\n",
    "                Process a single window (patch) by reading it, transforming it, running the model on it,\n",
    "                and writing the result to the output file.\n",
    "                \"\"\"\n",
    "                # Acquire the read lock to safely read from the input file\n",
    "                with read_lock:\n",
    "\n",
    "                    col_off = window.col_off - overlap\n",
    "                    row_off = window.row_off - overlap\n",
    "                    width = patchSize + overlap * 2\n",
    "                    height = patchSize + overlap * 2\n",
    "                    \n",
    "                    # Create a window with overlap\n",
    "                    overlap_window = Window(\n",
    "                        col_off=col_off,\n",
    "                        row_off=row_off,\n",
    "                        width=width,\n",
    "                        height=height\n",
    "                    )\n",
    "                    \n",
    "                    # Read the data from the input file within the overlap window\n",
    "                    src_array = src.read(boundless=True, window=overlap_window, fill_value = 0.0)\n",
    "                    src_array = torch.from_numpy(src_array)\n",
    "\n",
    "                    # Apply the image transformations\n",
    "                    image = imgTransforms({\"image\": src_array})['image']#.squeeze()\n",
    "\n",
    "                    def d4_transformations(image):\n",
    "                        # Generate the eight \\(\\text{D}_4\\) transformations\n",
    "                        transformations = [\n",
    "                            image,                                # Original\n",
    "                            F.rotate(image, 90),                  # Rotate 90 degrees\n",
    "                            F.rotate(image, 180),                 # Rotate 180 degrees\n",
    "                            F.rotate(image, 270),                 # Rotate 270 degrees\n",
    "                            F.hflip(image),                       # Horizontal flip\n",
    "                            F.vflip(image),                       # Vertical flip\n",
    "                            F.vflip(F.rotate(image, 90)),         # Vertical flip + Rotate 90 degrees\n",
    "                            F.hflip(F.rotate(image, 90))          # Horizontal flip + Rotate 90 degrees\n",
    "                        ]\n",
    "                        return torch.stack(transformations)\n",
    "                    \n",
    "                    def inverse_d4_transformations(outputs):\n",
    "                        # Define the inverse transformations for D4\n",
    "                        inverses = [\n",
    "                            lambda x: x,                          # Original\n",
    "                            lambda x: F.rotate(x, -90),           # Rotate -90 degrees\n",
    "                            lambda x: F.rotate(x, -180),          # Rotate -180 degrees\n",
    "                            lambda x: F.rotate(x, -270),          # Rotate -270 degrees\n",
    "                            lambda x: F.hflip(x),                 # Horizontal flip\n",
    "                            lambda x: F.vflip(x),                 # Vertical flip\n",
    "                            lambda x: F.rotate(F.vflip(x), -90),  # Rotate -90 degrees after vertical flip\n",
    "                            lambda x: F.rotate(F.hflip(x), -90)   # Rotate -90 degrees after horizontal flip\n",
    "                        ]\n",
    "                        # Apply each inverse transformation to the corresponding output\n",
    "                        return torch.stack([inverses[i](output) for i, output in enumerate(outputs)])\n",
    "\n",
    "\n",
    "                    # Apply D4 transformations to create a batch\n",
    "                    # d4_batch = d4_transformations(image)\n",
    "\n",
    "                    # Move batch to the device\n",
    "                    # d4_batch = d4_batch.to(device, dtype=torch.float)\n",
    "                    image = image.to(device, dtype=torch.float)\n",
    "\n",
    "                    patches, grid_size = split_tensor_to_patches(image, patchSize, overlap)\n",
    "\n",
    "                    # Set the model to evaluation mode\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        # Run the model on the padded image\n",
    "                        # softmax = torch.nn.Softmax2d()\n",
    "                        output = model(torch.nan_to_num(patches, nan=0.0, neginf=0.0, posinf=0.0))\n",
    "\n",
    "                        # Apply the inverse transformations\n",
    "                        # outputs = inverse_d4_transformations(output)\n",
    "                        \n",
    "                        # Take the mean of the batch along the batch dimension\n",
    "                        # output = outputs.mean(dim=0)\n",
    "                        # Split into patches\n",
    "    \n",
    "                    # Reconstruct\n",
    "                    output_tensor = reconstruct_from_patches(\n",
    "                        output,\n",
    "                        grid_size,\n",
    "                        (1, 2, 512, 512),\n",
    "                        patchSize,\n",
    "                        overlap\n",
    "                    )\n",
    "\n",
    "                    # Remove overlap and/or padding from the output (assuming padding in the original code)\n",
    "                    # Adjust final output extraction for your specific dimensions and padding\n",
    "                    result = torch.argmax(output_tensor[:, :, overlap:patchSize + overlap, overlap:patchSize + overlap], dim=1).squeeze().detach().cpu()\n",
    "                    \n",
    "                # Acquire the write lock to safely write to the output file\n",
    "                with write_lock:\n",
    "                    dst.write(result.numpy(), 1, window=window)\n",
    "            \n",
    "            # Use a ThreadPoolExecutor to process the windows in parallel\n",
    "            with tqdm(total=len(windows), desc=os.path.basename(outfile)) as pbar:\n",
    "                with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                    futures = {executor.submit(process, window): window for window in windows}\n",
    "                    \n",
    "                    try:\n",
    "                        for future in as_completed(futures):\n",
    "                            future.result()  # Wait for the future to complete\n",
    "                            pbar.update(1)  # Update the progress bar\n",
    "                    except Exception as ex:\n",
    "                        logger.error('Error during inference: %s', ex)\n",
    "                        executor.shutdown(wait=False, cancel_futures=True)\n",
    "                        raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping\\src\\components\\artifacts\\segScenes\\predictions\\predScene4.tif\"\n",
    "# filename2 = r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\data\\indonesia\\fire_data\\images\\L8_100066_010919_001.tif\"\n",
    "filename = r\"C:\\Users\\coach\\myfiles\\postdoc\\Fire\\code\\Burn_Area_Mapping\\src\\components\\artifacts\\segScenes\\20240219T082009_20240219T084413_T34HCJ.tif\"\n",
    "inference(infile = filename, imgTransforms= data_transform, model = model, outfile = dd, patchSize = 256, overlap = 128, num_workers=1, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches shape: torch.Size([9, 14, 256, 256])\n",
      "Grid size: (3, 3)\n",
      "Modified patches shape: torch.Size([9, 2, 256, 256])\n",
      "Reconstructed shape: torch.Size([1, 2, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.signal.windows import triang  # Import the correct function\n",
    "import matplotlib.pyplot as plt  # for optional visualization\n",
    "\n",
    "# Spline window function\n",
    "def _spline_window(window_size, power=2):\n",
    "    \"\"\"\n",
    "    Squared spline window function for smooth transition.\n",
    "    \"\"\"\n",
    "    intersection = int(window_size / 4)\n",
    "    wind_outer = (abs(2 * triang(window_size)) ** power) / 2  # Use correct function\n",
    "    wind_outer[intersection:-intersection] = 0\n",
    "\n",
    "    wind_inner = 1 - (abs(2 * (triang(window_size) - 1)) ** power) / 2\n",
    "    wind_inner[:intersection] = 0\n",
    "    wind_inner[-intersection:] = 0\n",
    "\n",
    "    wind = wind_inner + wind_outer\n",
    "    wind = wind / np.average(wind)\n",
    "    return wind\n",
    "\n",
    "cached_2d_windows = dict()\n",
    "\n",
    "def _window_2D(window_size, power=2):\n",
    "    \"\"\"\n",
    "    Generate and return a 2D spline window.\n",
    "    \"\"\"\n",
    "    global cached_2d_windows\n",
    "    key = \"{}_{}\".format(window_size, power)\n",
    "    if key in cached_2d_windows:\n",
    "        wind = cached_2d_windows[key]\n",
    "    else:\n",
    "        wind = _spline_window(window_size, power)\n",
    "        wind = np.outer(wind, wind)  # Create a 2D window by outer product\n",
    "        cached_2d_windows[key] = wind\n",
    "    return torch.tensor(wind, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def split_tensor_to_patches(tensor, patch_size=256, overlap=128):\n",
    "    \"\"\"\n",
    "    Efficiently split a tensor into overlapping patches using unfold.\n",
    "    \"\"\"\n",
    "    stride = patch_size - overlap\n",
    "    patches = tensor.unfold(2, patch_size, stride).unfold(3, patch_size, stride)\n",
    "    batch, channels, h_steps, w_steps, h_patch, w_patch = patches.shape\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(-1, channels, patch_size, patch_size)\n",
    "    return patches, (h_steps, w_steps)\n",
    "\n",
    "# def reconstruct_from_patches(patches, grid_size, output_size, patch_size=256, overlap=128, power=2):\n",
    "#     \"\"\"\n",
    "#     Reconstruct tensor from patches with smooth blending using a spline window.\n",
    "#     \"\"\"\n",
    "#     batch_size, out_channels, height, width = output_size\n",
    "#     h_steps, w_steps = grid_size\n",
    "#     stride = patch_size - overlap\n",
    "    \n",
    "#     # Create reconstruction tensor\n",
    "#     reconstructed = torch.zeros(output_size, device=patches.device)\n",
    "#     counter = torch.zeros_like(reconstructed)\n",
    "    \n",
    "#     # Create spline weight matrix\n",
    "#     weight = _window_2D(patch_size, power=power).to(patches.device)\n",
    "    \n",
    "#     # Calculate number of patches per batch\n",
    "#     patches_per_batch = h_steps * w_steps\n",
    "    \n",
    "#     # Process each batch\n",
    "#     for b in range(batch_size):\n",
    "#         batch_patches = patches[b * patches_per_batch:(b + 1) * patches_per_batch]\n",
    "        \n",
    "#         # Process each patch\n",
    "#         for idx in range(patches_per_batch):\n",
    "#             # Calculate grid position\n",
    "#             i = idx // w_steps\n",
    "#             j = idx % w_steps\n",
    "            \n",
    "#             y_start = i * stride\n",
    "#             x_start = j * stride\n",
    "            \n",
    "#             # Apply weight and add to reconstruction\n",
    "#             current_patch = batch_patches[idx].unsqueeze(0)  # Add batch dimension [1, C, H, W]\n",
    "#             weighted_patch = current_patch * weight  # Apply spline-based weight\n",
    "            \n",
    "#             reconstructed[b:b+1, :, y_start:y_start + patch_size, \n",
    "#                           x_start:x_start + patch_size] += weighted_patch\n",
    "#             counter[b:b+1, :, y_start:y_start + patch_size, \n",
    "#                     x_start:x_start + patch_size] += weight\n",
    "    \n",
    "#     # Normalize by counter to blend overlapping regions\n",
    "#     reconstructed = reconstructed / (counter + 1e-8)\n",
    "    \n",
    "#     return reconstructed\n",
    "\n",
    "def reconstruct_from_patches(patches, grid_size, output_size, patch_size=256, overlap=128, power=2):\n",
    "    \"\"\"\n",
    "    Reconstruct tensor from patches with smooth blending using a spline window.\n",
    "    \"\"\"\n",
    "    batch_size, out_channels, height, width = output_size\n",
    "    h_steps, w_steps = grid_size\n",
    "    stride = patch_size - overlap\n",
    "    \n",
    "    # Create reconstruction tensor\n",
    "    reconstructed = torch.zeros(output_size, device=patches.device)\n",
    "    counter = torch.zeros_like(reconstructed)\n",
    "    \n",
    "    # Create spline weight matrix\n",
    "    window_2d = _window_2D(patch_size, power=power).to(patches.device)\n",
    "    \n",
    "    # Calculate number of patches per batch\n",
    "    patches_per_batch = h_steps * w_steps\n",
    "    \n",
    "    # Process each batch\n",
    "    for b in range(batch_size):\n",
    "        batch_patches = patches[b * patches_per_batch:(b + 1) * patches_per_batch]\n",
    "        \n",
    "        # Process each patch\n",
    "        for idx in range(patches_per_batch):\n",
    "            # Calculate grid position\n",
    "            i = idx // w_steps\n",
    "            j = idx % w_steps\n",
    "            \n",
    "            y_start = i * stride\n",
    "            x_start = j * stride\n",
    "            \n",
    "            # Apply weight and add to reconstruction\n",
    "            current_patch = batch_patches[idx].unsqueeze(0)  # Add batch dimension [1, C, H, W]\n",
    "            \n",
    "            # Create a 2D window map the same size as the patch\n",
    "            patch_window = window_2d.repeat(1, out_channels, 1, 1)\n",
    "            \n",
    "            # Apply the 2D window to the patch\n",
    "            weighted_patch = current_patch * patch_window\n",
    "            \n",
    "            # Add the weighted patch to the reconstruction tensor, with proper positioning\n",
    "            reconstructed[b:b+1, :, y_start:y_start + patch_size, \n",
    "                          x_start:x_start + patch_size] += weighted_patch\n",
    "            \n",
    "            # Keep track of the total window weights at each pixel\n",
    "            counter[b:b+1, :, y_start:y_start + patch_size, \n",
    "                    x_start:x_start + patch_size] += patch_window\n",
    "    \n",
    "    # Normalize by counter to blend overlapping regions\n",
    "    reconstructed = reconstructed / (counter + 1e-8)\n",
    "    \n",
    "    return reconstructed\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample tensor\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sample_tensor = torch.randn(1, 14, 512, 512, device=device)\n",
    "    \n",
    "    # Parameters\n",
    "    patch_size = 256\n",
    "    overlap = 128\n",
    "    \n",
    "    # Split into patches\n",
    "    patches, grid_size = split_tensor_to_patches(sample_tensor, patch_size, overlap)\n",
    "    print(f\"Patches shape: {patches.shape}\")\n",
    "    print(f\"Grid size: {grid_size}\")\n",
    "    \n",
    "    # Simulate channel dimension change (14 -> 3)\n",
    "    modified_patches = patches[:, :2]\n",
    "    print(f\"Modified patches shape: {modified_patches.shape}\")\n",
    "    \n",
    "    # Reconstruct\n",
    "    reconstructed = reconstruct_from_patches(\n",
    "        modified_patches,\n",
    "        grid_size,\n",
    "        (1, 2, 512, 512),\n",
    "        patch_size,\n",
    "        overlap\n",
    "    )\n",
    "    print(f\"Reconstructed shape: {reconstructed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax2d(torch.randn(1, 2, 1, 1))\n",
    "print(m)\n",
    "# you softmax over the 2nd dimension\n",
    "input = \n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.signal.windows import triang  # Import the correct function\n",
    "import matplotlib.pyplot as plt  # for optional visualization\n",
    "\n",
    "# Spline window function\n",
    "def _spline_window(window_size, power=2):\n",
    "    \"\"\"\n",
    "    Squared spline window function for smooth transition.\n",
    "    \"\"\"\n",
    "    intersection = int(window_size / 4)\n",
    "    wind_outer = (abs(2 * triang(window_size)) ** power) / 2  # Use correct function\n",
    "    wind_outer[intersection:-intersection] = 0\n",
    "\n",
    "    wind_inner = 1 - (abs(2 * (triang(window_size) - 1)) ** power) / 2\n",
    "    wind_inner[:intersection] = 0\n",
    "    wind_inner[-intersection:] = 0\n",
    "\n",
    "    wind = wind_inner + wind_outer\n",
    "    wind = wind / np.average(wind)\n",
    "    return wind\n",
    "\n",
    "cached_2d_windows = dict()\n",
    "\n",
    "def _window_2D(window_size, power=2):\n",
    "    \"\"\"\n",
    "    Generate and return a 2D spline window.\n",
    "    \"\"\"\n",
    "    global cached_2d_windows\n",
    "    key = \"{}_{}\".format(window_size, power)\n",
    "    if key in cached_2d_windows:\n",
    "        wind = cached_2d_windows[key]\n",
    "    else:\n",
    "        wind = _spline_window(window_size, power)\n",
    "        wind = np.outer(wind, wind)  # Create a 2D window by outer product\n",
    "        cached_2d_windows[key] = wind\n",
    "    return torch.tensor(wind, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def _rotate_mirror_do(tensor):\n",
    "    \"\"\"\n",
    "    Apply all possible rotations and reflections to a tensor.\n",
    "    Returns a list of 8 transformed tensors.\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    transforms.append(tensor)\n",
    "    transforms.append(torch.rot90(tensor, 1, (2, 3)))\n",
    "    transforms.append(torch.rot90(tensor, 2, (2, 3)))\n",
    "    transforms.append(torch.rot90(tensor, 3, (2, 3)))\n",
    "    transforms.append(tensor.flip(2))\n",
    "    transforms.append(transforms[-1].rot90(1, (2, 3)))\n",
    "    transforms.append(transforms[-1].rot90(2, (2, 3)))\n",
    "    transforms.append(transforms[-1].rot90(3, (2, 3)))\n",
    "    return transforms\n",
    "\n",
    "def _rotate_mirror_undo(transformed_tensors):\n",
    "    \"\"\"\n",
    "    Undo the transformations applied by `_rotate_mirror_do`.\n",
    "    Returns the average of the transformed tensors.\n",
    "    \"\"\"\n",
    "    untransformed = []\n",
    "    for t in range(len(transformed_tensors)):\n",
    "        patch = transformed_tensors[t]\n",
    "        patch = patch.reshape(patch.shape[1], patch.shape[2], patch.shape[3])\n",
    "        untransformed.append(patch)\n",
    "\n",
    "    untransformed[1] = untransformed[1].rot90(1, (0, 1))\n",
    "    untransformed[2] = untransformed[2].rot90(2, (0, 1))\n",
    "    untransformed[3] = untransformed[3].rot90(3, (0, 1))\n",
    "    untransformed[4] = untransformed[4].flip(1)\n",
    "    untransformed[5] = untransformed[5].rot90(1, (0, 1))\n",
    "    untransformed[6] = untransformed[6].rot90(2, (0, 1))\n",
    "    untransformed[7] = untransformed[7].rot90(3, (0, 1))\n",
    "\n",
    "    return torch.mean(torch.stack(untransformed, dim=0), dim=0)\n",
    "\n",
    "def split_tensor_to_patches(tensor, patch_size=256, overlap=128):\n",
    "    \"\"\"\n",
    "    Efficiently split a tensor into overlapping patches using unfold.\n",
    "    \"\"\"\n",
    "    stride = patch_size - overlap\n",
    "    patches = tensor.unfold(2, patch_size, stride).unfold(3, patch_size, stride)\n",
    "    batch, channels, h_steps, w_steps, h_patch, w_patch = patches.shape\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(-1, channels, patch_size, patch_size)\n",
    "    return patches, (h_steps, w_steps)\n",
    "\n",
    "def reconstruct_from_patches(patches, grid_size, output_size, patch_size=256, overlap=128, power=2):\n",
    "    \"\"\"\n",
    "    Reconstruct tensor from patches with smooth blending using a spline window.\n",
    "    \"\"\"\n",
    "    batch_size, out_channels, height, width = output_size\n",
    "    h_steps, w_steps = grid_size\n",
    "    stride = patch_size - overlap\n",
    "    \n",
    "    # Create reconstruction tensor\n",
    "    reconstructed = torch.zeros(batch_size, out_channels, height, width, device=patches.device)\n",
    "    counter = torch.zeros_like(reconstructed)\n",
    "    \n",
    "    # Create spline weight matrix\n",
    "    window_2d = _window_2D(patch_size, power=power).to(patches.device)\n",
    "    \n",
    "    # Calculate number of patches per batch\n",
    "    patches_per_batch = h_steps * w_steps\n",
    "    \n",
    "    # Process each batch\n",
    "    for b in range(batch_size):\n",
    "        batch_patches = patches[b * patches_per_batch:(b + 1) * patches_per_batch]\n",
    "        \n",
    "        # Apply D4 transformations to each patch\n",
    "        transformed_patches = _rotate_mirror_do(batch_patches)\n",
    "        \n",
    "        # Process each patch\n",
    "        for idx in range(patches_per_batch):\n",
    "            # Calculate grid position\n",
    "            i = idx // w_steps\n",
    "            j = idx % w_steps\n",
    "            \n",
    "            y_start = i * stride\n",
    "            x_start = j * stride\n",
    "            \n",
    "            # Undo the transformations and compute the mean\n",
    "            untransformed_patches = _rotate_mirror_undo([transformed_patches[t][idx] for t in range(len(transformed_patches))])\n",
    "            patch_mean = untransformed_patches.unsqueeze(0)\n",
    "            \n",
    "            # Create a 2D window map the same size as the patch\n",
    "            patch_window = window_2d.repeat(1, out_channels, 1, 1)\n",
    "            \n",
    "            # Apply the 2D window to the patch\n",
    "            weighted_patch = patch_mean * patch_window\n",
    "            \n",
    "            # Add the weighted patch to the reconstruction tensor, with proper positioning\n",
    "            reconstructed[b:b+1, :, y_start:y_start + patch_size, \n",
    "                          x_start:x_start + patch_size] += weighted_patch\n",
    "            \n",
    "            # Keep track of the total window weights at each pixel\n",
    "            counter[b:b+1, :, y_start:y_start + patch_size, \n",
    "                    x_start:x_start + patch_size] += patch_window\n",
    "    \n",
    "    # Normalize by counter to blend overlapping regions\n",
    "    reconstructed = reconstructed / (counter + 1e-8)\n",
    "    \n",
    "    return reconstructed\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample tensor\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sample_tensor = torch.randn(1, 14, 512, 512, device=device)\n",
    "    \n",
    "    # Parameters\n",
    "    patch_size = 256\n",
    "    overlap = 128\n",
    "    \n",
    "    # Split into patches\n",
    "    patches, grid_size = split_tensor_to_patches(sample_tensor, patch_size, overlap)\n",
    "    print(f\"Patches shape: {patches.shape}\")\n",
    "    print(f\"Grid size: {grid_size}\")\n",
    "    \n",
    "    # Simulate channel dimension change (14 -> 3)\n",
    "    modified_patches = patches[:, :2]\n",
    "    print(f\"Modified patches shape: {modified_patches.shape}\")\n",
    "    \n",
    "    # Reconstruct\n",
    "    reconstructed = reconstruct_from_patches(\n",
    "        modified_patches,\n",
    "        grid_size,\n",
    "        (1, 2, 512, 512),\n",
    "        patch_size,\n",
    "        overlap\n",
    "    )\n",
    "    print(f\"Reconstructed shape: {reconstructed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional delete image \n",
    "# Optional upload after inference or wait all inference is completed\n",
    "# upload to earth engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def inference(infile, imgTransforms, model, outfile, patchSize, num_workers=4, device:str = None):\n",
    "#     \"\"\"\n",
    "#     Run inference using model on infile block-by-block and write to a new file (outfile). \n",
    "#     In the case, that the infile image width/height is not exactly divisible by 32, padding\n",
    "#     is added for inference and removed prior to the outfile being saved.\n",
    "    \n",
    "#     Args:\n",
    "#         infile (string): Path to input image/covariates\n",
    "#         model (pth file): Loaded trained model/checkpoint\n",
    "#         outfile (string): Path to save predicted image\n",
    "#         patchSize (int): Must be a multiple of 32. Size independent of model input size.\n",
    "#         num_workers (int): Num of workers to parralelise across\n",
    "        \n",
    "#     Returns:\n",
    "#         A tif saved to the outfile destination\n",
    "        \n",
    "#     \"\"\"\n",
    "\n",
    "#     with rio.open(infile) as src:\n",
    "        \n",
    "#         logger = logging.getLogger(__name__)\n",
    "\n",
    "#         # Create a destination dataset based on source params. The\n",
    "#         # destination will be tiled, and we'll process the tiles\n",
    "#         # concurrently.\n",
    "#         profile = src.profile\n",
    "#         profile.update(blockxsize= patchSize, blockysize= patchSize, tiled=True, count=1)\n",
    "\n",
    "#         with rio.open(Path(outfile), \"w\", **profile) as dst:\n",
    "#             windows = [window for ij, window in dst.block_windows()]\n",
    "\n",
    "#             # use a lock to protect the DatasetReader/Writer\n",
    "#             read_lock = threading.Lock()\n",
    "#             write_lock = threading.Lock()\n",
    "\n",
    "#             def process(window):\n",
    "#                 with read_lock:\n",
    "#                     src_array = src.read(window=window)#nbands, nrows, ncols(4, h, w)\n",
    "#                     w, h = src_array.shape[1], src_array.shape[2]\n",
    "#                     image = imgTransforms({\"image\": torch.from_numpy(src_array)})['image']\n",
    "#                     image = image.to(device, dtype=torch.float)#(1, h, w, 4)\n",
    "#                     hpad = math.ceil(h/32)*32-h\n",
    "#                     wpad = math.ceil(w/32)*32-w\n",
    "#                     transform = transforms.Pad((0, 0, hpad, wpad))\n",
    "#                     # add padding to image\n",
    "#                     image = transform(image)\n",
    "#                     model.eval()\n",
    "#                     output = model(image)[:, 1, :, :].squeeze()#(1,1,h,w)\n",
    "#                     # remove padding\n",
    "#                     result = output[0:w, 0:h].detach().cpu()\n",
    "#                     # plt.imshow(result.numpy())\n",
    "\n",
    "#                 with write_lock:\n",
    "#                     dst.write(result, 1, window=window)\n",
    "\n",
    "#             # We map the process() function over the list of\n",
    "#             # windows.\n",
    "#             with tqdm(total=len(windows), desc = os.path.basename(outfile)) as pbar:\n",
    "#                 with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "#                     futures = {executor.submit(process, window): window for window in windows}\n",
    "                    \n",
    "#                     try:\n",
    "#                         for future in concurrent.futures.as_completed(futures):\n",
    "#                             future.result()\n",
    "#                             pbar.update(1)\n",
    "                                    \n",
    "#                     except Exception as ex:\n",
    "#                         logger.info('Cancelling...')\n",
    "#                         executor.shutdown(wait=False, cancel_futures=True)\n",
    "#                         raise ex\n",
    "\n",
    "# check which scenes need to be downloaded\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/geethen/fire/Burn_Area_Mapping\")\n",
    "from src.utils import load_object\n",
    "from src.utils import load_object\n",
    "import ee\n",
    "import os\n",
    "import csv\n",
    "from geeml.utils import eeprint\n",
    "from geedim.download import BaseImage\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "from dataclasses import dataclass\n",
    "from torchgeo.transforms import AugmentationSequential\n",
    "from torchgeo.transforms import indices\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ee.Authenticate()\n",
    "ee.Initialize(project='ee-gsingh')\n",
    "\n",
    "supportedSensors = {'Sentinel-1': 'COPERNICUS/S1_GRD',\n",
    "                    'Sentinel-2': \"COPERNICUS/S2_SR_HARMONIZED\",\n",
    "                    'LANDSAT_8': \"LANDSAT/LC08/C02/T1_L2\",\n",
    "                    'LANDSAT_9': \"LANDSAT/LC09/C02/T1_L2\"}\n",
    "\n",
    "# Landsat data exploration\n",
    "def cloudMask1(image):\n",
    "    clouds = image.select('QA_PIXEL').bitwiseAnd(0b1000).eq(0)\n",
    "    cirrus = image.select('QA_PIXEL').bitwiseAnd(0b1100).eq(0)\n",
    "    saturationMask = image.select('QA_RADSAT').eq(0)\n",
    "\n",
    "    #   Apply the scaling factors to the appropriate bands.\n",
    "    opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n",
    "\n",
    "    #  Replace the original bands with the scaled ones and apply the masks.\n",
    "    return opticalBands\\\n",
    "        .updateMask(clouds)\\\n",
    "        .updateMask(cirrus)\\\n",
    "        .updateMask(saturationMask)\n",
    "\n",
    "def cloudMask2(image):\n",
    "    clouds = image.select('QA_PIXEL').bitwiseAnd(0b1000).eq(0)\n",
    "    cirrus = image.select('QA_PIXEL').bitwiseAnd(0b1100).eq(0)\n",
    "    saturationMask = image.select('QA_RADSAT').eq(0)\n",
    "\n",
    "    #   Apply the scaling factors to the appropriate bands.\n",
    "    opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n",
    "\n",
    "    # Compute normalised burn ratio (NBR)\n",
    "    nbr = image.normalizedDifference(['SR_B5', 'SR_B7']).rename('nbr')\n",
    "\n",
    "    #  Replace the original bands with the scaled ones and apply the masks.\n",
    "    return opticalBands.addBands([nbr], None, True)\\\n",
    "        .updateMask(clouds)\\\n",
    "        .updateMask(cirrus)\\\n",
    "        .updateMask(saturationMask)\n",
    "\n",
    "def preprocessOptical(image, sensor):\n",
    "    # Step 1) select bands\n",
    "    bandSelect = {'Sentinel-2': 'B.*',\n",
    "                    'LANDSAT_8': 'SR_B.',\n",
    "                    'LANDSAT_9': 'SR_B.'}\n",
    "    # Step 2) radiometric correction\n",
    "    radiometricMultiplySelect = {'Sentinel-2': 0.0001,\n",
    "                    'LANDSAT_8': 0.0000275,\n",
    "                    'LANDSAT_9': 0.0000275}\n",
    "    radiometricAddSelect = {'Sentinel-2': 0,\n",
    "                    'LANDSAT_8': -0.2,\n",
    "                    'LANDSAT_9': -0.2}\n",
    "    # processed = image.select(bandSelect.get(sensor)).multiply(radiometricMultiplySelect.get(sensor)).add(radiometricAddSelect.get(sensor))\n",
    "\n",
    "    # Step 3) Cloud mask\n",
    "    processed = cloudMask1(image)\n",
    "\n",
    "    # Step 4) Compute NBR\n",
    "    if sensor == 'Sentinel-2':\n",
    "        wIndices = processed.addBands([processed.normalizedDifference(['B8A', 'B12']).rename('nbr'), processed.normalizedDifference(['B8', 'B4']).rename('ndvi')])\n",
    "    if sensor == 'LANDSAT_8' or sensor == 'LANDSAT_9':\n",
    "        wIndices = processed.addBands([processed.normalizedDifference(['SR_B5', 'SR_B7']).rename('nbr'), processed.normalizedDifference(['SR_B5', 'SR_B4']).rename('ndvi')])\n",
    "    return wIndices\n",
    "\n",
    "def qualityMosaicPercentile(collection, QAband, percentile):\n",
    "    # Compute percentile image\n",
    "    percentileImage = collection.select(QAband).reduce(ee.Reducer.percentile([percentile]))\n",
    "    #  Compute distance of every pixel from the computed percentile in that location\n",
    "    withDist = ee.ImageCollection(collection.map(lambda image: image.addBands([\n",
    "        image.select(QAband).subtract(percentileImage).abs().multiply(-1).rename('quality')])))\n",
    "    return withDist.qualityMosaic('quality')\n",
    "\n",
    "def prepareInferenceImage(imageID: str, sensor:str)-> ee.Image:\n",
    "    \"\"\"Given an Image ID that is predicted (by the scene classification model) to contain a burn area,\n",
    "    download the required input data for the semantic segmentation model\n",
    "    \n",
    "    Args:\n",
    "        imageID (str): earth engine image ID\n",
    "        sensor (str): An optical sensor. Either 'Sentinel-2', 'LANDSAT_8' or 'LANDSAT_9'.\n",
    "        \n",
    "    Returns:\n",
    "        ee.Image with 16 bands for the before and after image\n",
    "        \n",
    "        \"\"\"\n",
    "    eventImage = ee.Image.load(f'{supportedSensors.get(sensor)}/{imageID}')\n",
    "    # radiometric calibration, compute NBR\n",
    "    postImggeom = preprocessOptical(eventImage, sensor)# mask clouds\n",
    "    eventDate = eventImage.date()\n",
    "    startDate = eventDate\n",
    "    endDate = eventDate.advance(15, 'week')\n",
    "    postImages = ee.ImageCollection(supportedSensors.get(sensor)).filterBounds(postImggeom.geometry()).filterDate(startDate, endDate) \n",
    "    postImages = postImages.map(lambda img: preprocessOptical(img, sensor))\n",
    "\n",
    "    startDate = eventDate.advance(15*-1, 'week')\n",
    "    endDate = eventDate\n",
    "    # Filter to relevant pre-fire images\n",
    "    images = ee.ImageCollection(supportedSensors.get(sensor)).filterBounds(postImggeom.geometry()).filterDate(startDate, endDate)\n",
    "    # filter images, mask clouds, compute nbr and ndvi, compute compsoite based on 95th percentile NDVI. subtract from postNBR.\n",
    "    if sensor == 'LANDSAT_8' or sensor == 'LANDSAT_9':\n",
    "        # preImg = ee.Image(qualityMosaicPercentile(images.map(lambda img: preprocessOptical(\n",
    "        #     img, sensor)),'ndvi', 95))\n",
    "        preImg = images.map(lambda img: preprocessOptical(img, sensor)).sort('system:time_start', False).reduce(ee.Reducer.firstNonNull()).regexpRename('.{6}$','')\n",
    "        postImage = qualityMosaicPercentile(postImages.map(lambda img: img.addBands(\n",
    "            preImg.normalizedDifference(['SR_B5', 'SR_B7']).subtract(img.normalizedDifference(['SR_B5', 'SR_B7'])).rename('dnbr'))), 'dnbr', 95)\n",
    "    \n",
    "        # dnbr = preImg.select('nbr').subtract(postImg.select('nbr')).rename('dnbr')\n",
    "        bands = ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'nbr', 'dnbr']\n",
    "        outimg = postImage.select(bands)\n",
    "    # else:\n",
    "        # preImg = ee.Image(qualityMosaicPercentile(ee.ImageCollection(images.map(lambda img: preprocessOptical(\n",
    "        #     img, sensor))),'ndvi', 95))\n",
    "        # preImg = images.map(lambda img: preprocessOptical(img, sensor)).sort('system:time_start', False).reduce(ee.Reducer.firstNonNull()).regexpRename('.{6}$','')\n",
    "        # dnbr = preImg.select('nbr').subtract(postImg.select('nbr')).rename('dnbr')\n",
    "        # bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8A', 'B11', 'B12', 'nbr', 'dnbr']\n",
    "        # outimg = postImg.addBands(dnbr).select(bands)\n",
    "    \n",
    "    return outimg, postImggeom\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class segModelConfig:\n",
    "    downloadList_path = os.path.join(Path.cwd().parent,'components/artifacts',\"downloadList.pkl\")\n",
    "    model_path = os.path.join(Path.cwd().parent,'components/artifacts',\"saft_segformer_gabam_r1_18032025.pth\")\n",
    "    error_log_path: str = os.path.join(Path.cwd().parent, 'components/artifacts', 'error_log.csv')\n",
    "\n",
    "class segment():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inference_config = segModelConfig()\n",
    "        self.initialize_error_log()\n",
    "\n",
    "    def initialize_error_log(self):\n",
    "        # Initialize the error log CSV file with headers\n",
    "        with open(self.inference_config.error_log_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Filename', 'Error'])\n",
    "\n",
    "    def log_error(self, filename, error_message):\n",
    "        # Append an error message to the error log CSV file\n",
    "        with open(self.inference_config.error_log_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([filename, error_message])\n",
    "    def main(self):\n",
    "        # Load the list of scenes to be downloaded\n",
    "        downloadList = load_object(self.inference_config.downloadList_path)\n",
    "\n",
    "        if len(downloadList) > 0:\n",
    "            # Define the directory paths\n",
    "            seg_scenes_dir = Path.cwd().parent / 'components/artifacts/segScenes'\n",
    "            predictions_dir = seg_scenes_dir / 'predictions'\n",
    "            seg_scenes_dir.mkdir(parents=True, exist_ok=True)\n",
    "            predictions_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Load the model\n",
    "            device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "            model = smp.Segformer(\n",
    "                encoder_name='mit_b4',\n",
    "                encoder_weights='imagenet',\n",
    "                in_channels=11,\n",
    "                classes=2\n",
    "            ).to(device)\n",
    "            checkpoint = torch.load(self.inference_config.model_path)\n",
    "            model.load_state_dict(checkpoint)\n",
    "\n",
    "            # Create transforms\n",
    "            data_transform = AugmentationSequential(\n",
    "                indices.AppendNDBI(index_swir=5, index_nir=3),\n",
    "                indices.AppendNDWI(index_green=1, index_nir=3),\n",
    "                indices.AppendNDVI(index_nir=3, index_red=2),\n",
    "                data_keys=['image']\n",
    "            )\n",
    "\n",
    "            # Process each image\n",
    "            for img in tqdm(['00LC08_174083_20160203']):\n",
    "                img_id = img[2:]\n",
    "                download_path = seg_scenes_dir / f\"{img_id}.tif\"\n",
    "                prediction_path = predictions_dir / f\"pred_{img_id}.tif\"\n",
    "\n",
    "                try:\n",
    "                    # Prepare image for download and inference\n",
    "                    eeImg, geom = prepareInferenceImage(img_id, 'LANDSAT_8')\n",
    "\n",
    "                    # Download scene if it doesn't exist\n",
    "                    if not prediction_path.exists():\n",
    "                        BaseImage(eeImg).download(\n",
    "                            str(download_path), crs=\"EPSG:4326\", region = geom.geometry(),\n",
    "                            scale=30, overwrite=True, num_threads=20, dtype=\"float64\"\n",
    "                        )\n",
    "                    else:\n",
    "                        print(f\"File {download_path} already exists. Skipping download.\")\n",
    "\n",
    "                    # Run inference if prediction doesn't exist\n",
    "                    if not prediction_path.exists():\n",
    "                        inference(\n",
    "                            infile=str(download_path), imgTransforms=data_transform,\n",
    "                            model=model, outfile=str(prediction_path), patchSize=256,\n",
    "                            num_workers=10, device=device\n",
    "                        )\n",
    "                    else:\n",
    "                        print(f\"File {prediction_path} already exists. Skipping inference.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    error_message = str(e)\n",
    "                    print(f\"Error processing {img_id}: {error_message}\")\n",
    "                    self.log_error(img_id, error_message)\n",
    "\n",
    "                # finally:\n",
    "                #     # Delete the downloaded image\n",
    "                #     try:\n",
    "                #         if download_path.exists():\n",
    "                #             os.remove(download_path)\n",
    "                #             print(f\"Deleted {download_path}\")\n",
    "                #     except Exception as e:\n",
    "                #         error_message = f\"Failed to delete {download_path}: {str(e)}\"\n",
    "                #         print(error_message)\n",
    "                #         self.log_error(img_id, error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/geethen/fire/Burn_Area_Mapping\")\n",
    "from src.components.inference import Inference\n",
    "\n",
    "inference_pipeline = Inference()\n",
    "ecoregions = ee.FeatureCollection(\"RESOLVE/ECOREGIONS/2017\")\n",
    "aoi = ee.FeatureCollection(ecoregions.filter(ee.Filter.inList('ECO_ID', [89,90]))).bounds()\n",
    "sceneList = inference_pipeline.initiate_inference_pipeline('LANDSAT_8', aoi, '2016-01-01', '2017-01-01')\n",
    "sceneList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_object, save_object\n",
    "last_checked = load_object(r\"/home/geethen/fire/Burn_Area_Mapping/src/components/artifacts/lastCheckedDate.pkl\")\n",
    "print(\"input dates\", last_checked)\n",
    "\n",
    "# overwrite last_checked with current date\n",
    "# save_object(r\"/home/geethen/fire/Burn_Area_Mapping/src/components/artifacts/lastCheckedDate.pkl\", ee.Date('2016-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ee\n",
    "try:\n",
    "    ee.Initialize(project='ee-gsingh',\n",
    "    opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project='ee-gsingh',\n",
    "    opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4048559/248987203.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.inference_config.model_path)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918d6472d6f14b1b9d439fe00453f15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:geedim.download:Consider adjusting `region`, `scale` and/or `dtype` to reduce the LC08_174083_20160203.tif download size (raw: 4.52 GB).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd66852f9ebb436fb8e9fbae9050fb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LC08_174083_20160203.tif: |          | 0.00/4.52G (raw) [  0.0%] in 00:00 (eta:     ?)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3977ac39b7334b45be65bc290519b352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pred_LC08_174083_20160203.tif:   0%|          | 0/1116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    segment().main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "Map = geemap.Map()\n",
    "Map.add_raster('/home/geethen/fire/Burn_Area_Mapping/src/components/artifacts/segScenes/predictions/pred_LC08_167079_20160101.tif')\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
