{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ece20f-aaa0-44b9-92f9-dc1a5175e247",
   "metadata": {},
   "source": [
    "# Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70ffd2c-7020-4207-ba4b-dc02e38390b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import geemap\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from geeml.utils import eeprint\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "881dbce2-390d-4f22-be05-2da41d8c8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get South Africa geometry\n",
    "countries = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\n",
    "geometry = ee.Geometry.Point([24.06353794842853, -29.732969740562062])\n",
    "sa = countries.filterBounds(geometry)\n",
    "sa_geo = sa.geometry()\n",
    "\n",
    "# Get Landsat data\n",
    "dataset = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\\\n",
    ".filterBounds(sa_geo)\\\n",
    ".filterDate('2018-06-28', '2023-08-22')\n",
    "\n",
    "# Applies scaling factors.\n",
    "def applyScaleFactors(image):\n",
    "    opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n",
    "    thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0)\n",
    "    return image.addBands(opticalBands, None, True).addBands(thermalBands, None, True)\n",
    "\n",
    "lS8 = dataset.map(applyScaleFactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6228a0af-4cce-408d-b43b-07ba62fc2a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>BurnDate</th>\n",
       "      <th>BurnYear</th>\n",
       "      <th>fireRisk</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>risk</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.119015</td>\n",
       "      <td>-33.452137</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>233</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.121262</td>\n",
       "      <td>-33.452137</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.119015</td>\n",
       "      <td>-33.449894</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.121262</td>\n",
       "      <td>-33.449894</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.116770</td>\n",
       "      <td>-33.447647</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  BurnDate  BurnYear  fireRisk          x          y risk  \\\n",
       "0  224879.0       231      2020    6555.0  27.119015 -33.452137  Low   \n",
       "1  224879.0       233      2020    6555.0  27.121262 -33.452137  Low   \n",
       "2  224879.0       231      2020    6555.0  27.119015 -33.449894  Low   \n",
       "3  224879.0       231      2020    6555.0  27.121262 -33.449894  Low   \n",
       "4  224879.0       231      2020    6555.0  27.116770 -33.447647  Low   \n",
       "\n",
       "        date  \n",
       "0 2020-08-18  \n",
       "1 2020-08-20  \n",
       "2 2020-08-18  \n",
       "3 2020-08-18  \n",
       "4 2020-08-18  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:/Users/coach/Documents/scratch/Post_doc/Fire/data/fire_2020_08_01.csv')\n",
    "df = df.dropna()\n",
    "risk_dict = {7692: 'Extreme', 7805:'High', 2903:'Medium', 6555:'Low'}\n",
    "df['risk'] = df['fireRisk'].astype(int).map(risk_dict)\n",
    "\n",
    "# Combine 'year' and 'day_of_year' columns to create a new datetime column\n",
    "df['date'] = pd.to_datetime(df['BurnYear'].astype(str) + df['BurnDate'].astype(str), format='%Y%j')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af84d00b-8573-471b-9dcf-be2ccd7fabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34579213/dbscan-for-clustering-of-geographic-location-data\n",
    "# https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/\n",
    "coords = df[['x', 'y']].values\n",
    "db = DBSCAN(eps=2/6371., min_samples=1, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1e1210-b818-4f07-afa1-34bb2a58f8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311     3662\n",
       "1325    2819\n",
       "1244    1493\n",
       "765     1437\n",
       "108     1378\n",
       "        ... \n",
       "1216       1\n",
       "492        1\n",
       "556        1\n",
       "1074       1\n",
       "1256       1\n",
       "Name: clusters, Length: 1475, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clusters'] = db.labels_\n",
    "df['clusters'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f7c832-9c15-4f11-ae34-e7061a4032c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>BurnDate</th>\n",
       "      <th>BurnYear</th>\n",
       "      <th>fireRisk</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>risk</th>\n",
       "      <th>date</th>\n",
       "      <th>clusters</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.119015</td>\n",
       "      <td>-33.452137</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>233</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.121262</td>\n",
       "      <td>-33.452137</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.119015</td>\n",
       "      <td>-33.449894</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.121262</td>\n",
       "      <td>-33.449894</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224879.0</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>27.116770</td>\n",
       "      <td>-33.447647</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  BurnDate  BurnYear  fireRisk          x          y risk  \\\n",
       "0  224879.0       231      2020    6555.0  27.119015 -33.452137  Low   \n",
       "1  224879.0       233      2020    6555.0  27.121262 -33.452137  Low   \n",
       "2  224879.0       231      2020    6555.0  27.119015 -33.449894  Low   \n",
       "3  224879.0       231      2020    6555.0  27.121262 -33.449894  Low   \n",
       "4  224879.0       231      2020    6555.0  27.116770 -33.447647  Low   \n",
       "\n",
       "        date  clusters start_date   end_date  duration  \n",
       "0 2020-08-18         0 2020-08-13 2020-08-21         9  \n",
       "1 2020-08-20         0 2020-08-13 2020-08-21         9  \n",
       "2 2020-08-18         0 2020-08-13 2020-08-21         9  \n",
       "3 2020-08-18         0 2020-08-13 2020-08-21         9  \n",
       "4 2020-08-18         0 2020-08-13 2020-08-21         9  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by 'group' column and calculate the required values\n",
    "grouped = df.groupby('clusters').agg(\n",
    "    start_date=('date', 'min'),\n",
    "    end_date=('date', 'max'),\n",
    "    duration=('date', lambda x: (x.max() - x.min()).days + 1)\n",
    ").reset_index()\n",
    "\n",
    "# Merge the grouped data back into the original dataframe\n",
    "result_df = df.merge(grouped, on='clusters')\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865db8f3-566c-42fc-b56d-918de3906ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a GeoDataFrame\n",
    "geometry = gpd.points_from_xy(df['x'], df['y'])\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs = 'EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af99952-08da-49c2-99b5-5bea1b0bee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'BurnDate', 'BurnYear', 'fireRisk', 'x', 'y', 'risk', 'date',\n",
    "       'clusters', 'start_date', 'end_date', 'duration', 'img_id',\n",
    "       'areakm2_BA', 'CO_column_number_density_p25_variance_p25',\n",
    "       'CO_column_number_density_p25_variance_p5',\n",
    "       'CO_column_number_density_p25_variance_p50',\n",
    "       'CO_column_number_density_p25_variance_p75',\n",
    "       'CO_column_number_density_p25_variance_p95',\n",
    "       'CO_column_number_density_p25_variance_p99',\n",
    "       'CO_column_number_density_p50_variance_p25',\n",
    "       'CO_column_number_density_p50_variance_p5',\n",
    "       'CO_column_number_density_p50_variance_p50',\n",
    "       'CO_column_number_density_p50_variance_p75',\n",
    "       'CO_column_number_density_p50_variance_p95',\n",
    "       'CO_column_number_density_p50_variance_p99',\n",
    "       'CO_column_number_density_p5_variance_p25',\n",
    "       'CO_column_number_density_p5_variance_p5',\n",
    "       'CO_column_number_density_p5_variance_p50',\n",
    "       'CO_column_number_density_p5_variance_p75',\n",
    "       'CO_column_number_density_p5_variance_p95',\n",
    "       'CO_column_number_density_p5_variance_p99',\n",
    "       'CO_column_number_density_p75_variance_p25',\n",
    "       'CO_column_number_density_p75_variance_p5',\n",
    "       'CO_column_number_density_p75_variance_p50',\n",
    "       'CO_column_number_density_p75_variance_p75',\n",
    "       'CO_column_number_density_p75_variance_p95',\n",
    "       'CO_column_number_density_p75_variance_p99',\n",
    "       'CO_column_number_density_p95_variance_p25',\n",
    "       'CO_column_number_density_p95_variance_p5',\n",
    "       'CO_column_number_density_p95_variance_p50',\n",
    "       'CO_column_number_density_p95_variance_p75',\n",
    "       'CO_column_number_density_p95_variance_p95',\n",
    "       'CO_column_number_density_p95_variance_p99',\n",
    "       'CO_column_number_density_p99_variance_p25',\n",
    "       'CO_column_number_density_p99_variance_p5',\n",
    "       'CO_column_number_density_p99_variance_p50',\n",
    "       'CO_column_number_density_p99_variance_p75',\n",
    "       'CO_column_number_density_p99_variance_p95',\n",
    "       'CO_column_number_density_p99_variance_p99']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f7484d-4524-4662-859e-ee45bfa919d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad2ecc7994044b09cf6f390369e2fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Y\n",
    "# For each fire duration (cluster),\n",
    "# get spatio-temporally overlapping Landsat scenes,\n",
    "# Get burn area extent for scene based on MODIS\n",
    "# add MODIS burn area (km2) as property to landsat scene.\n",
    "\n",
    "# Define a function to remove fires with confidence level < 50%\n",
    "def conf_mask(img):\n",
    "    conf = img.select('ConfidenceLevel')\n",
    "    level = conf.gt(50)\n",
    "    return img.updateMask(level).select('BurnDate').setDefaultProjection(**{'crs':projection, 'scale': 250})\n",
    "        \n",
    "outdf = pd.DataFrame(columns = cols)\n",
    "for cluster in tqdm(list(result_df.clusters.unique())):\n",
    "    # Get row from cluster\n",
    "    row = result_df.loc[result_df['clusters'] == cluster].iloc[0]\n",
    "    # Get start and end date\n",
    "    startDate = ee.Date(str(row['start_date'].date()))\n",
    "    endDate = ee.Date(str(row['end_date'].date()))\n",
    "    # For point extract data\n",
    "    x = row['x']\n",
    "    y = row['y']\n",
    "    point = ee.Geometry.Point([x, y])\n",
    "\n",
    "    # Filter Landsat for fire duration\n",
    "    ds = lS8.filterBounds(point).filterDate(startDate, endDate)\n",
    "    # Check if landsat scene exists\n",
    "    try:\n",
    "        dsSize = int(ds.size().getInfo())\n",
    "    except:\n",
    "        dsSize = 0\n",
    "    if dsSize>0:\n",
    "        t = ds.map(lambda img: img.set('id', img.id()))\n",
    "        row['img_id'] = t.aggregate_array('id').getInfo()[0]\n",
    "\n",
    "        # If scene not already in dataset\n",
    "        while (outdf.shape[0]==0) | (row['img_id'] not in outdf['img_id'].unique()): \n",
    "        \n",
    "            # Filter and map the fire collection\n",
    "            fire = ee.ImageCollection(\"ESA/CCI/FireCCI/5_1\")\n",
    "            # Set projection and scale\n",
    "            projection = fire.select('BurnDate').first().projection()\n",
    "            startDOY = startDate.getRelative('day', 'year')\n",
    "            endDOY = endDate.getRelative('day', 'year')\n",
    "            year = startDate.get('year')\n",
    "            month = startDate.get('month')\n",
    "            date = ee.Date.fromYMD(year, month, 1)\n",
    "            fire = fire \\\n",
    "                .filterBounds(point)\\\n",
    "                .filterDate(date, date.advance(1, 'month')) \\\n",
    "                .map(conf_mask)\\\n",
    "                .map(lambda img: img.gte(startDOY).And(img.lte(endDOY)).unmask(0))\\\n",
    "                .max().selfMask()\n",
    "        \n",
    "            area = ee.Image.pixelArea().divide(1e6)\n",
    "            burnAreaImage = area.multiply(fire).rename('areakm2_BA')\n",
    "            burnArea = burnAreaImage.reduceRegion(reducer = ee.Reducer.sum(),\n",
    "                                                  geometry = t.geometry(),\n",
    "                                                  scale = 250).get(\"areakm2_BA\")\n",
    "            row['areakm2_BA'] = burnArea.getInfo()\n",
    "    \n",
    "            # X\n",
    "            # For each landsat scene compute percentiles for x prior weeks, extract data\n",
    "            \n",
    "            date = ee.Image(t.first()).date()\n",
    "            \n",
    "            CO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\\\n",
    "            .filterBounds(t.geometry()).select('CO_column_number_density')\n",
    "            reducer = ee.Reducer.percentile([5, 25, 50, 75, 95, 99])\n",
    "            weeks = ee.List(list(range(-8, 0)))\n",
    "            # Get temporal percentiles per week\n",
    "            outic = weeks.map(lambda week: CO.filterDate(date.advance(week, 'week'), date.advance(ee.Number(week).add(1), 'week')).reduce(reducer).clip(t.geometry()))\n",
    "            # Compute variance across weeks\n",
    "            result = ee.Image(ee.ImageCollection(outic).reduce(ee.Reducer.variance())).regexpRename('^[^_]*_p', '')\n",
    "            # Compute spatial percentiles for variance image\n",
    "            predictors = result.reduceRegion(reducer = reducer,\n",
    "                                              geometry = t.geometry(),\n",
    "                                              scale = 1000)\n",
    "            row = row.append(pd.Series(predictors.getInfo(), name='predictors'))\n",
    "            row.name = str(row.id)\n",
    "            outdf = outdf.append(row)\n",
    "    # print(outdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11483d0-933a-455e-a3f9-f0b5f34cc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2_2019_08.csv\n",
    "outdf.to_csv(r'C:\\Users\\coach\\myfiles\\postdoc\\Fire\\data\\v2_2020_08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5ef51d-05c6-4eef-b055-20a727febfbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e907d13777649bb8239e279f64031e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract negative examples (no fires)\n",
    "\n",
    "# get all landsat scenes for SA for year and month of loaded modis fire pixels (df)\n",
    "\n",
    "year = df['date'][0].year\n",
    "month = df['date'][0].month\n",
    "date = ee.Date.fromYMD(year, month, 1)\n",
    "ds = lS8.filterDate(date, date.advance(1, 'month')).filterBounds(sa_geo).map(lambda img: img.set('id', img.id()))\n",
    "# get a list of scene ids\n",
    "ids = ds.aggregate_array('id').getInfo()\n",
    "\n",
    "noutdf = pd.DataFrame()\n",
    "for id in tqdm(ids):\n",
    "    outDict = {}\n",
    "    if id not in outdf['img_id']:\n",
    "        # set scene id\n",
    "        outDict['img_id'] = id \n",
    "        # set response (burn area) to 0\n",
    "        outDict['areakm2_BA'] = 0\n",
    "\n",
    "        # X\n",
    "        # For each landsat scene compute percentiles for x prior weeks, extract data\n",
    "\n",
    "        # Select Landsat scene\n",
    "        img = ee.Image(ds.filter(ee.Filter.eq('id', id)).first())\n",
    "        # get date of image\n",
    "        date = img.date()\n",
    "\n",
    "        # prepare predictors (carbon monoxide)\n",
    "        CO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\\\n",
    "        .filterBounds(img.geometry()).select('CO_column_number_density')\n",
    "        reducer = ee.Reducer.percentile([5, 25, 50, 75, 95, 99])\n",
    "        weeks = ee.List(list(range(-8, 0)))\n",
    "        outic = weeks.map(lambda week: CO.filterDate(date.advance(week, 'week'), date.advance(ee.Number(week).add(1), 'week')).reduce(reducer).clip(img.geometry()))\n",
    "        result = ee.Image(ee.ImageCollection(outic).reduce(ee.Reducer.variance())).regexpRename('^[^_]*_p', '')\n",
    "        predictors = result.reduceRegion(reducer = reducer,\n",
    "                                          geometry = img.geometry(),\n",
    "                                          scale = 1000).getInfo()\n",
    "        outDict.update(predictors)\n",
    "        noutdf = noutdf.append(pd.Series(outDict, name='instance'))\n",
    "    # print(noutdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab7327a-9616-48c6-8aaf-f9f33f143b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2_2019_08_neg.csv\n",
    "noutdf.reset_index(drop = True).to_csv(r'C:\\Users\\coach\\myfiles\\postdoc\\Fire\\data\\v2_2020_08_neg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26defdd-a659-4d4f-b8b2-d96f7e532903",
   "metadata": {},
   "source": [
    "### Exploration: Plot expanding window of percentiles prior to landsat image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848a9221-76a8-4825-be55-d4d3e27b41a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"ec0507bf-be6a-4641-812d-a0ac4f081bba\" style=\"height: auto; width:100%;\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " \n",
       "        <script src=\"/static/components/requirejs/require.js\"></script> <!-- Needed in Colab -->\n",
       "        <script>\n",
       "            require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
       "              renderjson.set_show_to_level(1)\n",
       "              document.getElementById('ec0507bf-be6a-4641-812d-a0ac4f081bba').appendChild(renderjson({\"type\": \"Image\", \"bands\": [{\"id\": \"CO_column_number_density_p95_variance\", \"data_type\": {\"type\": \"PixelType\", \"precision\": \"double\"}, \"crs\": \"EPSG:4326\", \"crs_transform\": [1, 0, 0, 0, 1, 0]}]}))\n",
       "            });\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X\n",
    "# For each landsat scene compute percentiles for x prior weeks, extract data\n",
    "\n",
    "date = ee.Image(t.first()).date()\n",
    "\n",
    "CO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\\\n",
    ".filterBounds(t.geometry()).select('CO_column_number_density')\n",
    "reducer = ee.Reducer.percentile([5, 25, 50, 75, 95, 99])\n",
    "weeks = ee.List(list(range(-8, 0)))\n",
    "outic = weeks.map(lambda week: CO.filterDate(date.advance(week, 'week'), date).reduce(reducer).clip(t.geometry()))\n",
    "result = ee.ImageCollection(outic).reduce(ee.Reducer.variance())\n",
    "result = result.select(\"CO_column_number_density_p95_variance\").subtract(result.select(\"CO_column_number_density_p5_variance\"))\n",
    "eeprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "865912cf-62fd-4fe2-bbb5-2f54e6a2810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = \"CO_column_number_density_p95_variance\"\n",
    "min = result.select(band).reduceRegion(reducer = ee.Reducer.min(),\n",
    "                                          geometry = t.geometry(),\n",
    "                                          scale = 1000).get(band)\n",
    "# eeprint(min)\n",
    "max = result.select(band).reduceRegion(reducer = ee.Reducer.max(),\n",
    "                                          geometry = t.geometry(),\n",
    "                                          scale = 1000).get(band)\n",
    "# eeprint(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e22500a-22b2-4879-acbe-baa2e05d0c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e01eb44598435aab5a2babd5262505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-33.83616638183594, 24.26237297058105], controls=(WidgetControl(options=['position', 'transparent_…"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Map = geemap.Map()\n",
    "Map.centerObject(point, 8)\n",
    "Map.addLayer(result, {'bands':band, 'min': min, 'max': max})\n",
    "Map.addLayer(point, {'color': 'red'})\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e27eba-59e2-4623-96bc-9e688f842315",
   "metadata": {},
   "source": [
    "## Wishlist\n",
    "- Merge fires across months\n",
    "- Try maxent\n",
    "- Try larger dataset\n",
    "- Calculate the lag upload for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fc517-636f-42af-801f-9d340939f831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erthy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
